{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3a/tLRgTI19RYLecTK2GC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chetankhairnar05/Python_Automation/blob/main/web_scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RYazUD5iMLke"
      },
      "outputs": [],
      "source": [
        "# This is the most reliable setup for Selenium in Google Colab.\n",
        "# It uses a library specifically designed to handle Colab's environment.\n",
        "# Run all these commands in a single cell in your Colab notebook.\n",
        "\n",
        "!pip install google-colab-selenium pandas openpyxl\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "from google_colab_selenium import Chrome\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "\n",
        "def scrape_webpage(url):\n",
        "    \"\"\"\n",
        "    Scrapes product information from a generic e-commerce search results page.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the webpage to scrape.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, where each dictionary contains\n",
        "              the data for one product.\n",
        "    \"\"\"\n",
        "    print(\"Initializing web driver with a Colab-specific stable setup...\")\n",
        "\n",
        "    # The Chrome() function from the library handles all the setup,\n",
        "    # making it much more reliable in Colab's environment.\n",
        "    try:\n",
        "        driver = Chrome()\n",
        "        driver.get(url)\n",
        "\n",
        "        # Allow more time for the page to load, especially for dynamic content on e-commerce sites\n",
        "        time.sleep(10)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to initialize WebDriver or load the page. Please check your URL and Colab environment. Error: {e}\")\n",
        "        return []\n",
        "\n",
        "    products_data = []\n",
        "\n",
        "    try:\n",
        "        print(\"Finding product cards...\")\n",
        "        # --- Using a robust attribute (data-id) which is less likely to change.\n",
        "        product_cards_xpath = '//div[@data-id]'\n",
        "\n",
        "        product_cards = driver.find_elements(By.XPATH, product_cards_xpath)\n",
        "\n",
        "        print(f\"Found {len(product_cards)} product cards.\")\n",
        "\n",
        "        if not product_cards:\n",
        "            print(\"No product cards were found with the specified XPath.\")\n",
        "            return []\n",
        "\n",
        "        # --- Using starts-with to find elements with similar class names ---\n",
        "        # This is more robust than contains() because it's less prone to finding false positives.\n",
        "        name_xpath = './/div[starts-with(@class, \"_4rR01T\")]'\n",
        "        current_price_xpath = './/div[starts-with(@class, \"_30jeq3\")]'\n",
        "        original_price_xpath = './/div[starts-with(@class, \"_3I9_wc\")]'\n",
        "        rating_xpath = './/div[starts-with(@class, \"_3LWZlK\")]'\n",
        "        rating_count_xpath = './/span[starts-with(@class, \"_2_R_DZ\")]'\n",
        "\n",
        "        for i, card in enumerate(product_cards):\n",
        "            data = {}\n",
        "            try:\n",
        "                # Scrape product name\n",
        "                name_element = card.find_element(By.XPATH, name_xpath)\n",
        "                data['name'] = name_element.text\n",
        "            except NoSuchElementException:\n",
        "                data['name'] = None\n",
        "            except Exception:\n",
        "                data['name'] = None\n",
        "\n",
        "            try:\n",
        "                # Scrape current price\n",
        "                price_element = card.find_element(By.XPATH, current_price_xpath)\n",
        "                data['price'] = price_element.text\n",
        "            except NoSuchElementException:\n",
        "                data['price'] = None\n",
        "            except Exception:\n",
        "                data['price'] = None\n",
        "\n",
        "            try:\n",
        "                # Scrape original price\n",
        "                original_price_element = card.find_element(By.XPATH, original_price_xpath)\n",
        "                data['original_price'] = original_price_element.text\n",
        "            except NoSuchElementException:\n",
        "                data['original_price'] = None\n",
        "            except Exception:\n",
        "                data['original_price'] = None\n",
        "\n",
        "            try:\n",
        "                # Scrape rating\n",
        "                rating_element = card.find_element(By.XPATH, rating_xpath)\n",
        "                data['rating'] = rating_element.text\n",
        "            except NoSuchElementException:\n",
        "                data['rating'] = None\n",
        "            except Exception:\n",
        "                data['rating'] = None\n",
        "\n",
        "            try:\n",
        "                # Scrape number of ratings\n",
        "                rating_count_element = card.find_element(By.XPATH, rating_count_xpath)\n",
        "                data['rating_count'] = rating_count_element.text\n",
        "            except NoSuchElementException:\n",
        "                data['rating_count'] = None\n",
        "            except Exception:\n",
        "                data['rating_count'] = None\n",
        "\n",
        "            if data['name']: # Only add products that have at least a name\n",
        "                products_data.append(data)\n",
        "                print(f\"Scraped product {i+1}: {data['name']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during scraping: {e}\")\n",
        "    finally:\n",
        "        print(\"Closing the browser...\")\n",
        "        driver.quit()\n",
        "\n",
        "    return products_data\n",
        "\n",
        "def save_to_excel(data, filename=\"scraped_products.xlsx\"):\n",
        "    \"\"\"\n",
        "    Saves a list of dictionaries to an Excel file.\n",
        "    \"\"\"\n",
        "    if data:\n",
        "        df = pd.DataFrame(data)\n",
        "        df.to_excel(filename, index=False)\n",
        "        print(f\"\\nData successfully saved to {os.path.abspath(filename)}\")\n",
        "    else:\n",
        "        print(\"\\nNo data to save.\")\n",
        "\n",
        "# --- Main script execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    webpage_url = \"https://www.flipkart.com/samsung-mobile-store?otracker=nmenu_sub_Electronics_0_Samsung\"\n",
        "    scraped_data = scrape_webpage(webpage_url)\n",
        "    if scraped_data:\n",
        "        save_to_excel(scraped_data)\n",
        "    else:\n",
        "        print(\"No product data was scraped.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2XUtdy7ng3NM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}